{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Matemáticas para Redes Neuronales\n",
    "\n",
    "Este notebook contiene ejemplos prácticos para entender los conceptos matemáticos detrás de las redes neuronales, incluyendo:\n",
    "\n",
    "- Operaciones lineales en una neurona\n",
    "- Cálculo de la función de coste y gradient descent\n",
    "- Funciones de activación y la introducción de no linealidad\n",
    "- Manejo de hiperparámetros (tasa de aprendizaje y batch size)\n",
    "\n",
    "Cada sección incluye ejemplos en Python para visualizar y experimentar con estos conceptos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Operación Lineal en una Neurona\n",
    "\n",
    "Una neurona calcula una combinación lineal de sus entradas y añade un sesgo. La fórmula es:\n",
    "\n",
    "\\( z = x_1 \\cdot w_1 + x_2 \\cdot w_2 + b \\)\n",
    "\n",
    "Luego, se aplica una función de activación (por ejemplo, sigmoide) para obtener la salida."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Definir entrada, pesos y sesgo\n",
    "x = np.array([1.0, 2.0])  # x1 = 1, x2 = 2\n",
    "w = np.array([0.5, -0.3]) # w1 = 0.5, w2 = -0.3\n",
    "b = 0.1                  # Sesgo\n",
    "\n",
    "# Operación lineal\n",
    "z = np.dot(x, w) + b\n",
    "print('Valor de z:', z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función de activación sigmoide\n",
    "def sigmoid(z):\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "a = sigmoid(z)\n",
    "print('Salida de la neurona (a):', a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Gradient Descent y Cálculo de la Función de Coste\n",
    "\n",
    "En un modelo de regresión lineal, la función de coste (MSE) se define como:\n",
    "\n",
    "\\( J(w, b) = \\frac{1}{2n}\\sum_{i=1}^{n} (y_i - (Xw + b))^2 \\)\n",
    "\n",
    "El gradiente descendente actualiza los parámetros según:\n",
    "\n",
    "\\( w := w - \\eta \\frac{\\partial J}{\\partial w} \\) y \\( b := b - \\eta \\frac{\\partial J}{\\partial b} \\)\n",
    "\n",
    "El siguiente ejemplo implementa gradient descent en un dataset sintético."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Generar datos sintéticos\n",
    "np.random.seed(42)\n",
    "n_samples = 50\n",
    "X = np.random.rand(n_samples, 2)\n",
    "true_w = np.array([2.0, -3.0])\n",
    "true_b = 1.0\n",
    "y = X.dot(true_w) + true_b + np.random.randn(n_samples) * 0.5\n",
    "\n",
    "# Inicializar parámetros\n",
    "np.random.seed(1)\n",
    "w = np.random.randn(2)\n",
    "b = np.random.randn()\n",
    "\n",
    "learning_rate = 0.05\n",
    "epochs = 100\n",
    "\n",
    "loss_history = []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    y_pred = X.dot(w) + b\n",
    "    loss = np.mean((y - y_pred)**2) / 2\n",
    "    loss_history.append(loss)\n",
    "    \n",
    "    error = y_pred - y\n",
    "    grad_w = (1/n_samples) * X.T.dot(error)\n",
    "    grad_b = (1/n_samples) * np.sum(error)\n",
    "    \n",
    "    w = w - learning_rate * grad_w\n",
    "    b = b - learning_rate * grad_b\n",
    "\n",
    "print('Pesos finales:', w)\n",
    "print('Sesgo final:', b)\n",
    "\n",
    "plt.figure(figsize=(8,5))\n",
    "plt.plot(range(epochs), loss_history, marker='o')\n",
    "plt.xlabel('Época')\n",
    "plt.ylabel('Loss (MSE)')\n",
    "plt.title('Evolución del Loss durante el Entrenamiento')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Funciones de Activación y No Linealidad\n",
    "\n",
    "Las funciones de activación introducen no linealidad, lo que permite a la red aprender patrones complejos. Sin ellas, una red neuronal se reduciría a una única transformación lineal. Se pueden usar funciones como sigmoide, ReLU o tanh. El siguiente ejemplo grafica estas funciones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "def relu(z):\n",
    "    return np.maximum(0, z)\n",
    "\n",
    "def tanh(z):\n",
    "    return np.tanh(z)\n",
    "\n",
    "z = np.linspace(-5, 5, 100)\n",
    "\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.plot(z, sigmoid(z), label='Sigmoide')\n",
    "plt.plot(z, relu(z), label='ReLU')\n",
    "plt.plot(z, tanh(z), label='Tanh')\n",
    "plt.xlabel('Entrada (z)')\n",
    "plt.ylabel('Salida')\n",
    "plt.title('Comparación de Funciones de Activación')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Manejo de Hiperparámetros: Tasa de Aprendizaje y Batch Size\n",
    "\n",
    "La tasa de aprendizaje \\(\\eta\\) controla el tamaño de los pasos en la actualización de pesos. Un valor muy alto puede causar inestabilidad, mientras que un valor muy bajo puede ralentizar el entrenamiento.\n",
    "\n",
    "El tamaño del batch determina cuántas muestras se usan para calcular el gradiente en cada actualización. Un batch pequeño puede introducir ruido, mientras que un batch grande proporciona una estimación más estable del gradiente.\n",
    "\n",
    "En el siguiente ejemplo se entrenan modelos con diferentes tasas de aprendizaje y se observa la evolución del loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generar un dataset sintético (100 muestras, 2 características)\n",
    "np.random.seed(42)\n",
    "n_samples = 100\n",
    "X = np.random.rand(n_samples, 2)\n",
    "true_w = np.array([2.0, -3.0])\n",
    "true_b = 1.0\n",
    "y = X.dot(true_w) + true_b + np.random.randn(n_samples) * 0.5\n",
    "\n",
    "# Probar con diferentes tasas de aprendizaje\n",
    "learning_rates = [0.001, 0.01, 0.1]\n",
    "losses_lr = {}\n",
    "\n",
    "epochs = 100\n",
    "\n",
    "for lr in learning_rates:\n",
    "    np.random.seed(1)\n",
    "    w = np.random.randn(2)\n",
    "    b = np.random.randn()\n",
    "    loss_history = []\n",
    "    for epoch in range(epochs):\n",
    "        y_pred = X.dot(w) + b\n",
    "        loss = np.mean((y - y_pred)**2) / 2\n",
    "        loss_history.append(loss)\n",
    "        error = y_pred - y\n",
    "        grad_w = (1/n_samples) * X.T.dot(error)\n",
    "        grad_b = (1/n_samples) * np.sum(error)\n",
    "        w = w - lr * grad_w\n",
    "        b = b - lr * grad_b\n",
    "    losses_lr[lr] = loss_history\n",
    "\n",
    "plt.figure(figsize=(10,6))\n",
    "for lr, loss_history in losses_lr.items():\n",
    "    plt.plot(range(epochs), loss_history, label=f'Learning Rate: {lr}')\n",
    "plt.xlabel('Época')\n",
    "plt.ylabel('Loss (MSE)')\n",
    "plt.title('Evolución del Loss para Diferentes Tasas de Aprendizaje')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusión\n",
    "\n",
    "En este notebook hemos visto cómo se aplican conceptos matemáticos fundamentales en machine learning:\n",
    "\n",
    "- **Operación lineal:** Producto punto y suma de sesgos en una neurona.\n",
    "- **Gradient descent:** Cálculo del error (MSE) y actualización de pesos y sesgos.\n",
    "- **Funciones de activación:** Introducen no linealidad para aprender relaciones complejas.\n",
    "- **Hiperparámetros:** Tasa de aprendizaje y tamaño del batch y su efecto en la convergencia.\n",
    "\n",
    "¡Experimenta modificando los parámetros para ver cómo cambian los resultados y refuerza tu comprensión de estos conceptos fundamentales!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "version": "3.x"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
